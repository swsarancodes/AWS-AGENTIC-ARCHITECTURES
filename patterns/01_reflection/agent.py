


from patterns.base.agent_base import AgentBase
from patterns.base.bedrock_client import BedrockClient
from typing import Dict, Any, List, Optional
import logging
import time

logger = logging.getLogger(__name__)


class ReflectionAgent(AgentBase):
    
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        
        super().__init__(config)
        
        self.bedrock = BedrockClient(
            region=self.config.get("region", "us-east-1")
        )
        
        self.model_id = self.config.get(
            "model_id",
            "anthropic.claude-3-sonnet-20240229-v1:0"
        )
        
        self.max_iterations = self.config.get("max_iterations", 3)
        self.min_quality_score = self.config.get("min_quality_score", 0.8)
        self.temperature = self.config.get("temperature", 0.7)
        
        logger.info(
            f"ReflectionAgent initialized: model={self.model_id}, "
            f"max_iterations={self.max_iterations}"
        )
    
    def run(self, input_data: str) -> Dict[str, Any]:
        """
        Execute reflection pattern on the input task.\n        \n        Args:\n            input_data: The task or prompt to process\n            \n        Returns:\n            Dictionary containing:\n            - final_output: The refined final result\n            - initial_output: The first draft\n            - iterations_used: Number of reflection cycles performed\n            - critiques: List of critiques from each iteration\n            - improvements: List of improvements made\n            - quality_scores: Quality scores over iterations\n            - execution_time: Total time taken\n            - status: Success/failure status\n        \"\"\"\n        start_time = time.time()\n        \n        try:\n            logger.info(f\"Starting reflection on task: {input_data[:100]}...\")\n            \n            # Step 1: Generate initial response\n            current_output = self._generate_initial_response(input_data)\n            initial_output = current_output\n            \n            # Track iteration history\n            critiques = []\n            improvements = []\n            quality_scores = []\n            \n            # Step 2: Iterative reflection and improvement\n            for iteration in range(self.max_iterations):\n                logger.info(f\"Reflection iteration {iteration + 1}/{self.max_iterations}\")\n                \n                # Critique current output\n                critique = self._critique_output(input_data, current_output)\n                critiques.append(critique)\n                \n                # Calculate quality score\n                quality_score = self._calculate_quality_score(critique)\n                quality_scores.append(quality_score)\n                \n                logger.debug(f\"Quality score: {quality_score:.2f}\")\n                \n                # Check if quality threshold met\n                if quality_score >= self.min_quality_score:\n                    logger.info(f\"Quality threshold reached: {quality_score:.2f}\")\n                    break\n                \n                # Generate improved version\n                improved_output = self._improve_output(\n                    input_data, current_output, critique\n                )\n                improvements.append(improved_output)\n                \n                current_output = improved_output\n            \n            execution_time = time.time() - start_time\n            \n            result = {\n                \"final_output\": current_output,\n                \"initial_output\": initial_output,\n                \"iterations_used\": len(critiques),\n                \"critiques\": critiques,\n                \"improvements\": improvements,\n                \"quality_scores\": quality_scores,\n                \"final_quality_score\": quality_scores[-1] if quality_scores else 0.0,\n                \"execution_time\": execution_time,\n                \"status\": \"success\",\n                \"metadata\": {\n                    \"model_id\": self.model_id,\n                    \"max_iterations\": self.max_iterations,\n                    \"min_quality_score\": self.min_quality_score\n                }\n            }\n            \n            # Track execution\n            self._track_execution(input_data, result, execution_time, True)\n            \n            logger.info(\n                f\"Reflection completed: {len(critiques)} iterations, \"\n                f\"final quality={quality_scores[-1]:.2f}, time={execution_time:.2f}s\"\n            )\n            \n            return result\n            \n        except Exception as e:\n            execution_time = time.time() - start_time\n            error_result = {\n                \"status\": \"error\",\n                \"error\": str(e),\n                \"execution_time\": execution_time\n            }\n            \n            self._track_execution(input_data, error_result, execution_time, False)\n            logger.error(f\"Reflection failed: {e}\")\n            \n            return error_result\n    \n    def _generate_initial_response(self, task: str) -> str:\n        \"\"\"\n        Generate the initial response to the task.\n        \n        Args:\n            task: The input task/prompt\n            \n        Returns:\n            Initial generated response\n        \"\"\"\n        prompt = f\"\"\"Please provide a comprehensive response to the following task:\n\nTask: {task}\n\nProvide a complete, well-structured response.\"\"\"\n        \n        response = self.bedrock.invoke_model(\n            model_id=self.model_id,\n            prompt=prompt,\n            temperature=self.temperature,\n            max_tokens=4096\n        )\n        \n        return response['completion']\n    \n    def _critique_output(self, task: str, output: str) -> Dict[str, Any]:\n        \"\"\"\n        Generate a critique of the current output.\n        \n        Args:\n            task: Original task\n            output: Current output to critique\n            \n        Returns:\n            Dictionary containing critique details\n        \"\"\"\n        critique_prompt = f\"\"\"You are a critical reviewer. Analyze the following response and provide constructive critique.\n\nOriginal Task: {task}\n\nResponse to Critique:\n{output}\n\nProvide your critique in the following format:\n1. Strengths: What works well\n2. Weaknesses: What needs improvement\n3. Specific Issues: Concrete problems to address\n4. Suggestions: How to improve\n5. Overall Quality: Rate from 0-10\n\nBe thorough and specific.\"\"\"\n        \n        response = self.bedrock.invoke_model(\n            model_id=self.model_id,\n            prompt=critique_prompt,\n            temperature=0.5,  # Lower temperature for more consistent critique\n            max_tokens=2048\n        )\n        \n        critique_text = response['completion']\n        \n        return {\n            \"text\": critique_text,\n            \"input_tokens\": response['input_tokens'],\n            \"output_tokens\": response['output_tokens']\n        }\n    \n    def _calculate_quality_score(self, critique: Dict[str, Any]) -> float:\n        \"\"\"\n        Calculate quality score from critique.\n        \n        Extracts the quality rating from the critique text and normalizes to 0-1.\n        \n        Args:\n            critique: Critique dictionary\n            \n        Returns:\n            Quality score between 0.0 and 1.0\n        \"\"\"\n        critique_text = critique['text'].lower()\n        \n        # Try to extract numerical score\n        import re\n        score_match = re.search(r'overall quality[:\\s]*(\\d+)[\\/\\s]*10', critique_text)\n        \n        if score_match:\n            score = float(score_match.group(1)) / 10.0\n            return min(max(score, 0.0), 1.0)  # Clamp to [0, 1]\n        \n        # Fallback: estimate based on keywords\n        positive_keywords = ['excellent', 'strong', 'good', 'clear', 'well-structured']\n        negative_keywords = ['weak', 'poor', 'unclear', 'lacks', 'missing', 'needs improvement']\n        \n        positive_count = sum(1 for word in positive_keywords if word in critique_text)\n        negative_count = sum(1 for word in negative_keywords if word in critique_text)\n        \n        # Normalized score\n        if positive_count + negative_count == 0:\n            return 0.5  # Neutral if no keywords found\n        \n        score = positive_count / (positive_count + negative_count)\n        return score\n    \n    def _improve_output(self, task: str, current_output: str, \n                       critique: Dict[str, Any]) -> str:\n        \"\"\"\n        Generate improved version based on critique.\n        \n        Args:\n            task: Original task\n            current_output: Current version\n            critique: Critique to address\n            \n        Returns:\n            Improved output\n        \"\"\"\n        improvement_prompt = f\"\"\"Improve the following response based on the critique provided.\n\nOriginal Task: {task}\n\nCurrent Response:\n{current_output}\n\nCritique:\n{critique['text']}\n\nPlease provide an improved version that addresses all the issues raised in the critique.\nMaintain the strengths while fixing the weaknesses. Provide a complete, polished response.\"\"\"\n        \n        response = self.bedrock.invoke_model(\n            model_id=self.model_id,\n            prompt=improvement_prompt,\n            temperature=self.temperature,\n            max_tokens=4096\n        )\n        \n        return response['completion']\n    \n    def get_reflection_summary(self, result: Dict[str, Any]) -> str:\n        \"\"\"\n        Generate a human-readable summary of the reflection process.\n        \n        Args:\n            result: Result dictionary from run()\n            \n        Returns:\n            Formatted summary string\n        \"\"\"\n        if result.get('status') != 'success':\n            return f\"Reflection failed: {result.get('error', 'Unknown error')}\"\n        \n        summary = f\"\"\"Reflection Summary:\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nIterations: {result['iterations_used']}/{self.max_iterations}\nFinal Quality Score: {result['final_quality_score']:.2f}\nExecution Time: {result['execution_time']:.2f}s\n\nQuality Progression:\n{' → '.join([f\"{score:.2f}\" for score in result['quality_scores']])}\n\nKey Improvements:\n{self._summarize_improvements(result['critiques'])}\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\"\"\n        \n        return summary\n    \n    def _summarize_improvements(self, critiques: List[Dict[str, Any]]) -> str:\n        \"\"\"Summarize key improvements from critiques.\"\"\"\n        if not critiques:\n            return \"No iterations performed\"\n        \n        improvements = []\n        for i, critique in enumerate(critiques, 1):\n            # Extract key points (simplified)\n            text = critique['text'][:200]\n            improvements.append(f\"  {i}. {text}...\")\n        \n        return \"\\n\".join(improvements)

